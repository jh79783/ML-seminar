# 아달린 로지스틱 회귀 모델

아달린을 로지스틱 회귀로 바꾸기 위해서는 비용함수 j를 새로운 비용함수로 바꾸면 된다. 새로운 J는 다음과같이 정의된다.
$$
J(w)=-\Sigma[y^ilog(\phi(z^i))+(1-y^i)log(1-\phi(z^i))]
$$
새롭게 정의된 비용함수로 에포크마다 모든 훈련 샘플을 분류하는 비용을 계산한다. 원래 아달린의 비용 함수는 $J(w)=\frac{1}{2}\Sigma(y^i-\phi(z^i))^2$으로 정의된다 즉, 원래의 아달린 비용함수에 로그를 취한 것이다. 또한 선형 활성화 함수를 시그모이드 활성화로 바꾸고 임계 함수가 클래스 레이블 -1과 1이 아닌 0과 1을 반환하도록 변경한다. 이 세가지를 반영한다.

```
 cost = (errors**2).sum() / 2.0
 self.cost_.append(cost)
```

원래 아달린 비용함수J이다. 이 코드는 오차를 제곱합 해주는데 이 대신 다음과 같이 코드를 수정한다.

```
cost = -y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))
self.cost_.append(cost)
```

위의 코드는 새로운 비용함수를 적용한 것이다. 

.dot은 내적을 한 것이고 np.log는 output을 로그함수로 변경한 것이다.

```
def activation(self, X):
	return X					// 선형 활성화 함수
	
def activation(self, z):
	return 1./(1.+np.exp(-np.clip(z, -250, 250)))	//로지스틱 시그모이드 활성화 
```

여기서 1뒤에 .이 붙는것은 정수 + 행렬, 정수 / 행렬이 불가능 하기 때문에 모든 행렬에 대해 수행하기 위해 .을 붙인다. np.exp 는 로그함수를 나타낸 것이며 np.clip은 범위를 제한하는 것이다. np.clip(z, -250, 250)는 즉 z의 크기를 -250~250까지 제한하는 것이다. 

따라서 위의 식은 z를 -250~250까지 제한한 값을 로그를 취한 값에 1씩 모두 더하고 그 값을 나누는 것이다. $\frac{1}{1+e^{-z}}$

# 사이킷런 로지스틱 회귀 모델

```
lr = LogisticRegression(solver='liblinear', multi_class='auto', C=100.0, random_state=1)
```

다음과 같은 코드가 있다. 여기서 LogisticRegression함수에 쓰인 변수는 다음과 같다.

- solver : 최적화에 사용할 알고리즘 결정('newton_cg', lbfgs', liblinear', 'sag', 'saga'
- multi_class : 분류 방식 ('str', 'ovr', multinomial', 'auto')
- C : 규칙의 강도의 역수 값
- random_state : 시드값

훈련 샘플이 어떤 클래스에 속할 확률은 predict_proba 메서드를 사용하여 계산한다.

실행 결과를 보면 첫번째 행에서 가장 큰값은 0.853으로 나와있다 이는 85.3%의 확률로 3번째 클래스에 속할 확률이라는 것이고, 이 열이 예측 클래스의 레이블이 된다. 이렇게 확룰로 보지 않고 다음 과같은 코드를 실행하면 클래스 인덱스로 확인할 수 있다. ` lr.predict(X_test_std[:3, :])`

샘플 하나의 클래스 레이블을 예측할 때 주의점이 있다. 사이킷런은 입력 데이터로 2차원 배열을 기대하기 때문에 하나의 행을 2차원 포맷으로 먼저 변경해야 합니다. 하나의 행을 2차원 배열로 변경하기 위해 넘파이의 reshape 메서드를 사용하여 새로운 차원을 추가한다.

위 파라미터에서 'C'라는 파라미터가 있다. 이부분을 알기위해서 머신러닝에 있어 공통적으로 나타나는 문제인 과대적합(overfitting)과 이를 해결하기 위한 정규화 라는 개념을 알아야 한다.

과대적합(overfitting)은 너무 잘 맞아 떨어진다는 뜻이며 입력된 값에만 너무 잘 맞아 떨어지도록 계산해 트레이닝 데이터 이외의 데이터들에 대해서는 잘 맞지 않는 경우가 발생하게되어 결과 예측이 잘 되지 않는 것을 말한다.

![](/home/moon/workspace/ML-seminar/overfitting_2.png)

두번째 그림처럼 최적의 조건을 찾기 위해 사용되는 것이 정규화이다. 정규화를 통헤 모델의 복잡성을 튜닝해 과소적합(underfitting)과 과대적합(overfitting)의 트레이드 오프를 찾는 것이다. 로지스틱 회귀에서 사용되는 정규화 기법은 가장 일반적으로 L2정규화이다. C는 L2정규화와 관련된 인자로 이해하면 된다.

L2 정규화는 로지스틱 회귀의 비용함수 $J(w)$를 다음과 같은 식으로 변형해 동작한다.
$$
J(w)_{reg}=J(w)+\frac{\lambda}{2}\sum_{j}w_j^2
$$
여기서 $\lambda$를 정규화 파라미터라고 부르며, C값은 $\lambda$의 역수로 정의한다.
$$
C=\frac{1}{\lambda}
$$
따라서 C값을 감소시키면 $\lambda$가 커지게 되며, 이는 정규화를 강하게 한다는 의미이다.

