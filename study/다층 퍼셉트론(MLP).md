# 다층 퍼셉트론(MLP)

다층 퍼셉트론은 복수의 퍼셉트론 계층을 둔 신경망 구조를 말한다.

![](C:\workspace\ML\image\SLP.png)

![](C:\workspace\ML\image\MLP2.jpeg)

위의 두 사진은 각각 단층 퍼셉트론, 다층 퍼셉트론의 한 예시이다.
전 장에서 공부했던 단층 퍼셉트론은 첫번째 사진의 은닉층이 없이 입력계층과 출력계층만 존재하는 것을 확인할 수 있다.
다층 퍼셉트론은 은닉 계층을 갖고있으며, 한 계층에 속한 퍼셉트론은 입력을 공유하며 출력을 만들어낸다. 하지만 이때 각각의 퍼셉트론마다 영향은 끼치지 않는다.
은닉 계층에서 폭(width)라는 것이 있는데, 폭은 한 계층이 갖는 퍼셉트론의 수를 말한다. 
또한 퍼셉트론 하나를 노드(node)라고 한다.

앞의  첫번째 그림에서 입력 벡터의 크기는 3, 출력 벡터의 크기는 1이다. 노드는 각각 4개를 갖고있다. 즉, 퍼셉트론 4개를 갖는 은닉층이 2개이다. 퍼셉트론은 출력계층에 1개, 은닉 계층에 각각4개씩 하여 총 9개의 퍼셉트론을 갖는다.
또한, 한 계층의 퍼셉트론들은 각각 입력 벡터 크기만큼의 가중치(weight)와 편향(bias) 파라미터를 갖는다. 따라서, 첫째 은닉 계층의 파라미터는 3(입력 갯수)\*4(퍼셉트론 갯수)+4(퍼셉트론 갯수)=16개의 파라미터를 갖고, 둘째 은닉 계층에는 4\*4+4=20개의 파라미터, 출력 계층에는 4*1+1=5개의 파라미터를 갖게 된다. 따라서 총16+20+5=41개의 파라미터를 갖게 된다. 

## 비선형 활성화 함수

전 장에서는 가중치와 편향을 이용하여 선형 계산된 결과값을 내보낸 후 softmax나 sigmoid를 사용하여 값을  출력하였다. 하지만 이번 장 MLP에서는 은닉 계층이 추가되었기 때문에, 은닉계층에서 계산된 결과값에 각각 장치를 추가해 적용한 뒤에 값을 내보낸다.
이렇게 계산된 결과값 뒤에 적용되어 출력을 변형시키려 추가한 장치를 비선형 활성화 함수라고 한다.
선형 연산은 항상 입력의 일차 함수로만 나타나는데, 비선형 함수는 일차 함수로는 표현이 불가능한 기능을 하는 함수이다. 따라서 비선형 활성화 함수를 추가하게 되면 더 복잡한 형태의 퍼셉트론 출력 형태를 만들 수 있다.
전에 배운 시그모이드와 소프트맥스 함수는 비선형 활성화 함수이지만 이들은 퍼셉트론 내의 구성 요소로 간주하지 않았다. 왜냐하면 출력 계층에서 출력의 유형(이진분류, 선택분류)에 따라 비선형 함수를 이용하는 방법이 달라지고, 이 때문에 비선형 함수를 퍼셉트론 안에 두는 것이 곤란하기 때문이다.
아래의 사진은 소프트맥스가 적용되는 위치를 보여준다. 사진에 나타난 소프트맥스는 여러개의 출력을 묶어 함께 처리하기 때문에 독립된 구조에서는 사용이 불가능하다. 

![](C:\workspace\ML\image\소프트맥스 작동.png)

출력 계층에서는 비선형 활성화 함수를 사용하지 않으며, 지난 시간에 사용한 시그모이드와 소프트맥스 함수는 후처리 과정에서 사용한 것일뿐, 출력 계층에서는 사용하지 않았다. 따라서 비선형 활성화 함수는 각 은닉계층의 사이에 비선형 활성화 함수가 놓이게 된다.

## 선형성의 한계

단층 퍼셉트론을 반복하면 다층 퍼셉트론이 가능 할까? 라는 질문에는 가능하지 않다.
왜냐하면 선형 함수로만 이루어진 은닉층을 매우 많이 만들어도, 이것은 은닉층이 없는 것과 똑같이 표현이 가능한 수학적 원리때문이다.
그 수식은 다음과 같이 표현할 수 있다.
$$
h_i = w_{i1}x_1+...+w_{in}x_n+b\\
y_j = v_{j1}h_1+...+v_{jm}h_m+c\\
y_j = (w_{11}v_{j1}+...+w_{m1}v_{jm}x_1)+...+(w_{1n}v_{j1}+...+w_{mn}v_{jm}x_n)+(v_{j1}b+...+v_{jm}b+c)
$$
선형계산은 일차함수이기 때문에 이들은 결국 비례관계에 있어 미분을 하게 되면 비례관계로 나타나게 된다. 또한 bias는 미분과정에서 사라져 의미가 없어지게 된다.

하지만 위의 선형적인 식에 비선형 함수$\phi$를 도입해 비례관계가 깨지기 때문에 선형성의 한계에서 벗어날 수 있다.
$$
h_i=\phi(w_{i1}x_1+...+w_{in}x_n+b)
$$

## ReLU 함수

ReLU(rectified linear unit) 함수는 음수 입력을 0으로 만들기 때문에, 은닉 계층의 비선형 활성화 함수로 가장 많이 사용된다.
$$
ReLU\;definition\\
x>0:y=x\\
x<=0:y=0
$$
![](C:\workspace\ML\image\relu.jpg)

ReLU함수의 한 가지 문제는 x=0에서 미분이 불가능 하다는 점이다.
따라서 이때, 대부분 x=0일때 기울기를 0으로 설정해 준다.
또한, 미분값을 x값 대신 y값을 사용하여 더 쉽게 계산할 수 있다.
$$
y>0:y'=1\\
y<=0:y'=0
$$
이것이 가능한 이유는 음수인 출력이 나올 수 없기 때문이다. 따라서 python 언어의 np.sign()함수는 양수와 0에 대해 각각 1과 0을 출력하기 때문에, ReLU함수의 미분은 np.sign(y)를 사용하여 쉽게 구할 수 있다.

## 민스키의 XOR문제

민스키는 단층 퍼셉트론에서 XOR를 구현할 수 없음을 나타내었다.

XOR표는 다음과 같다

|  x1  |  x2  |  y   |
| :--: | :--: | :--: |
|  0   |  0   |  0   |
|  0   |  1   |  1   |
|  1   |  0   |  1   |
|  1   |  1   |  0   |

입력이 두개인 퍼셉트론은 밑의 식을 만족하게 된다.
$$
y=w_1x_1+w_2x_2+b
$$
위의 식에서 $x_2=0$인 경우를 생각해보면, $x_1=0$이면 $y=0$이 되고, $x_1=1$이면 $y=1$이 되어야 한다. 하지만 파라미터는 각각의 퍼셉트론에만 영향을 끼칠뿐, 다른 퍼셉트론에 영향을 끼칠 수 없다. 따라서 $x_2$값에 따라 $x_1$과$y$값이 변할 수가 없다.
이를 해결하기 위해 비선형 활성화 함수를 넣은 다층 퍼셉트론을 만들어 XOR문제를 해결할 수 있게 되었다.
![](C:\workspace\ML\image\xor.jpg)

위의 신경망을 예로 들자.
이 신경망은 H1, H2 두개의 퍼셉트론으로 구성되며, 은닉계층이 1개인 신경망이다.
weight와 bias는 위와 같이 설정되어 있으며 은닉계층에서 나온 출력값을 $\gamma$로 표시된 ReLU함수를 통해 출력계층으로 제공되고있다.
x1,x2의 조합에 따라 l1,l2,h1,h2,y를 추적해보면 다음과 같다.

| x1   | x2   | l1   | l2   | h1   | h2   | y    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | 0    | 0    | -2   | 0    | 0    | 0    |
| 0    | 1    | 1    | 0    | 1    | 0    | 1    |
| 1    | 0    | 1    | 0    | 1    | 0    | 1    |
| 1    | 1    | 2    | 2    | 2    | 2    | 0    |

따라서 XOR문제는 다층 퍼셉트론에 비선형 활성화 함수를 추가하여 해결하였다.