# 1.1 단층 퍼셉트론(SLP)

단층 퍼셉트론은 퍼셉트론을 한줄로 배치하고 입력 벡터로부터 출력 벡터를 단번에 얻어내는 것을 말한다. 
입력 벡터로부터 출력 벡터를 얻어내려면 출력 벡터의 크기만큼 퍼셉트론이 필요하다.
퍼셉트론 사이에는 어떠한 연결도 되어 있지 않아 영향을 전혀 받지 않으며 퍼셉트론은 각각의 가중치와 편향값을 이용하여 출력 벡터를 계산하게 된다.

단층 퍼셉트론은 퍼셉트론들끼리 서로영향을 받지 않기 때문에 높은 수준의 능력을 기대하기 어려워 또 다른 퍼셉트론을 추가하기도 하는데, 이러한 추가된 퍼셉트론을 은닉 계층이라 부른다.
딥러닝에서 퍼셉트론 열을 계층(layer)이라 부르고 출력을 생성하는 계층을 출력 계층이라 부른다. 

# 1.2 텐서 연산과 미니배치

딥러닝에서의 텐서는 다차원 숫자 배열이라고 이해할 수 있다.
딥러닝에서 텐서가 중요한 이유는 반복문으로 처리해야할 문제를 텐서로 병렬처리하게 되면 빠르고 간단하게 효율적으로 처리할 수 있기 때문이다.
딥러닝에서의 미니배치는 일반적으로 신경망이 여러 데이터를 한번에 처리하는 것을 말한다.

<img src="https://t1.daumcdn.net/cfile/tistory/2514343C56C3280B11" style="zoom:50%;" />

위의 그림은 퍼셉트론 하나가 동작하는 방식이다. 가중치 벡터 **w**는 편향 스칼라 b를 이용해 출력을 계산한다.
이러한 계산을 반복문을 통하여 계산할 수 있지만 그보다 벡터의 내적을 이용하여 계산하는 것이 더욱 빠르다.

**x**=x~0~+x~1~+x~2~, **w**=w~0~+w~1~+w~2~, y=x~0~w~0~+x~1~w~1~+x~2~w~2~=**xw**와 같이 일차식으로 표현되는 이런 계산 과정을 선형 연산이라고 하며, 일차식으로 나타내지 못하는 계산 과정을 비선형 연산이라고 한다.

미니배치는 모든 학습 데이터를 한번에 일괄처리하는 작업은 아니지만 그렇다고 데이터를 하나씩 처리하지도 않는다. 일반적인 배치 작업보다 작은 단위로 처리하기때문에 미니배치라고 부르며 미니배치는 데이터 처리의 효율을 높여주며 각각의 학습 데이터의 특징을 무시하지 않으며, 특징에 너무 휘둘리지 않게 해주기 때문에 유용하다.

# 1.3 신경망의 세 가지 기본 출력 유형과 회귀 분석

인공지능 알고리즘의 출력 유형은 세가지가 있다.

1. 회귀 분석
2. 이진 판단
3. 선택 분류

회귀 분석은 어떤 특징값 하나를 숫자로 추정하여 출력해준다. 입력으로 주어진 값들은 근거로 미지의 변수값을 추정하고 예측하는데에 주로 사용되며 딥러닝 알고리즘의 값 추정 또한 회귀 분석에 해당된다.

이진 판단은 예, 아니오 둘중 하나를 선택하여 출력해주는 것이다.
선택 분류는 몇 가지 후보중 하나를 선택하여 결과를 출력해 준다.

# 1.4 회귀 분석과 평균제곱오차(MSE) 손실 함수

딥러닝에서 학습이 잘 될수록 더 정확한 추정값을 만든다. 이 추정값을 볼때 정답과 얼마나 정확한지 보여주는 정량적 지표가 필요하다.
회귀 분석에서는 추정값을 평가할때 보통 평균제곱오차(MSE)를 사용한다.
평균제곱오차는 간단히 말해 오차의 제곱에 대해 평균을 취한 것이다. 다시말해서 추정값과 정답 사이의 오차를 제곱하고 모두 더해 전체 성분 수로 나누는 것이다.
식은 다음과 같다.  $MSE = \frac{1}{n}\sum_{i=1}^n (\hat{Y}_i-Y_i)^2$ 여기서 $\hat{Y_i}$는 신경망의 출력(신경망이 추정한 값), Y~i~는 정답 레이블, i는 데이터의 차원 수 이다.
코드로는 다음과 같이 구현이 가능하다.

```python
((y^-y)**2).mean(axis=None)
```

MSE값은 오차를 제곱하기 때문에 항상 0 이상이며 추정이 정확해 질수록 0에 가까워 지게 된다. 추정값과 정답이 완전히 일치하게 되면 MSE는 0이 된다.

딥러닝에서는 미분도 가능한 평가지표를 정의한 후 이를  최소화 하는 것을 목표로 학습을 수행하게 된다. 이런 성질의 평가 지표를 손실 함수(loss function) 혹은 비용함수(cost function)라고 부른다.
평균제곱오차는 계산하기 쉬우며 딥러닝이 요구하는 성질을 충족시키기 때문에 손실함수로 유용하게 사용된다.

평균제곱오차 대신 평균오차를 사용하지 않는 이유는 평균오차는 미분이 불가능한 지점이 있고, 학습의 효과 또한 좋지 않기 때문에 사용하지 않는다.

다시 정리하자면 MSE는 항상 0 이상이며 추정값이 정답과 가까워 질수록 0에 근접하게 된다. 또한 단순한 평균 오차는 미분의 불가능한 지점의 존재와 학습 효과가 좋지 않기 때문에 단순한 평균 오차보다 평균제곱오차를 사용한다.

# 1.5 경사하강법과 역전파

경사하강법은 함수의 기울기를 반복 계산하면서 이 기울기에 따라 함숫값이 낮아지는 방향으로 이동하는 기본적인 학습 알고리즘이다.
선형회귀를 예로 일차함수식 y = wx + b 와같은 식을 쓴다면 최적의 w, b값을 찾을때 사용하는 방법이 경사하강법이다.

미니배치 입력 데이터에 대해 순전파와 역전파 과정을 반복하며 신경망 파라미터를 원하는 방향으로 바꾸어 나가는 것이다.
순전파는 입력된 데이터에 대해 신경망 구조를 따라가며 현재의 파라미터값들을 이용하여 손실 함수값을 계산하는 것을 말하며, 역전파는 순전파의 계산과정을 거꾸로 진행하며 손실 함수값을 계산하는 것을 말한다. 성분 x에 대한 손실 기울기는 그 성분에 대한 손실 함수값의 변화율을 말한다. 파라미터 성분데 대해서 계산된 손실 기울기를 이용하여 값을 변경한다. 이 변경으로 학습이 일어나게 된다. 경사하강법의 경우 학습률이라는 하이퍼파라미터값을 곱한 값을 빼는 방법으로 파라미터값을 변경하게 된다.

![](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99334E495D81B36B2F)

Starting Point에서 x값을 적당히 증가시키며 f(x)값이 최소가 되는 곳으로 다가가야 한다. 경사가 심한 곳에서는 목표값이 멀리 떨어져 있으니 빠르게 내려가도 된다. 경사가 완만한 경우 목표값이 가까워 천천히 움직여야 한다. 반대로 경사가 오르막이라면 x값을 줄여 뒤로 진행해야 한다.
그래프의 경사는 미분값f'(x)에 해당하게 된다. 따라서 이에 비례하는 값을 x에서 빼주는 처리를 하면 이 모든 처리가 가능하다.

위 그림을 식으로 나타나면 다음과 같다. $x_{i+1} = x_i-\alpha\frac{\sigma f(x)}{\sigma x}$가 된다. 이때의 $\alpha$를 비례상수 학습률이라고 하며 임의의 양수값을 사용할 수 있지만 값이 클수록 목표 근처에서 정확한 값을 찾는 능력이 무뎌지고 값이 작을 수록 정확한 값을 찾는데 시간이 오래 걸리게 된다.

식에서 전미분 대신 편미분을 사용한다. 전미분은 간접적으로 미치는 영향까지 모두 정확하게 따지는 반면에 편미분은 간접적인 영향은 무시한채 직접적인 영향만을 따지기 때문에 더 쉽게 계산할 수 있다.

기울기에 따라 보폭을 조절하는 복잡한 과정을 진행하는 이유는 최소화 해야할 손실 함수값을 결정하는 성분이 많기 때문이다. 현재의 위치를 제외하곤 주변이 어떻게 생겼는지 확인하지 못하게 때문에 현재위치의 기울기를 따져가며 내려갈 수 밖에 없다. 이때 내려가는 방향은$\theta_0$에 대한 기울기와$\theta_1$에 대한 기울기로 구성되는 기울기 벡터를 사용하여 결정해야 한다.

하지만 이것이 항상 최적의 바닥점에 도착할 수 있다는 보장이 없다. 주변 지형에 비해 상대적으로 낮은 바닥점을 찾을 수 있는 것이지 절대적으로 낮은 바닥점을 찾는 것이 아니며, 이것에 대한 많은 개선책과 보조 기법이 제안되고 있다.
ex) Batch Gradient Descent, SGD, Mini Batch Gradient Descent(Mini batch SGD)

# 1.6 편미분과 손실 기울기의 계산

편미분의 연쇄적 관계 덕분에 우리는 영향을 미친 모든 성분에 대해 손실 기울기를 계산할 수 있다.
편미분의 연쇄적 관계는 미분의 기본 성질인 다음 수식을 의미한다. $\frac{\sigma L}{\sigma x} = \frac{\sigma L}{\sigma y}\frac{\sigma y}{\sigma x}$이 식을 통하여 역전파의 진행과정을 알 수 있다.

순전파 처리과정에서 입력 x로부터 출력 y를 계산하는 과정이 있다고 하자. 그렇다면 순전파로부터 L이 계산되는 과정은 다음과 같다.

1. x가 구해진다
2. y=f(x)가 계산된다.
3. y를 이용하여 손실함수 L이 계산된다.

역전파는 위의 순전파의 반대 방향으로 진행되어야 한다. 역전파의 계산과정은 다음과 같다.

1. $\frac{\sigma L}{\sigma L}=1.0$으로 부터 y의 손실 기울기 $\frac{\sigma L}{\sigma y}$를 구해 값을 전달하게 된다.
2. $\frac{\sigma L}{\sigma x}=\frac{\sigma L}{\sigma y}\frac{\sigma y}{\sigma x}=\frac{\sigma L}{\sigma y}\frac{\sigma f(x)}{\sigma x}$로 부터 입력x의 손실 기울기 $\frac{\sigma L}{\sigma x}$를 전달받아 역전파를 처리하게 된다.

입력이 여러개인 경우는 같은 방법으로 계산하나 값들을 모아 전달하면 된다. $\frac{\sigma L}{\sigma x}={\frac{\sigma L}{\sigma x_1},...,\frac{\sigma L}{\sigma x_n}}$
하나의 출력이 중복으로 이용되는 경우도 있다. 이경우에도 위 처럼 출력의 손실 기울기를 단순히 합산하여 손실 기울기 계산에 반영하면 된다.

가중치나 편향 같은 파라미터 성분은 손실기울기에 학습률을 곱한 값을 빼주게 되므로써 그 값을 변경시킨다. 이 부분이 학습이 일어나는 부분이며 이때의 학습률은 학습 속도를 조절하는 매우 중요한 파라미터 값으로 적절한 값을 설정해주어야 한다.

# 1.7 하이퍼파라미터

1. 입력데이터

입력 데이터는 개발자 입장에서는 직접 손 댈수없는 고정된 값이다. 좋은 결과를 위해서는 잘 만들어진 데이터가 필요하다.

2. 파라미터

순전파나 역전파 처리과정에서 생산되는 각종 계산 결과값이다. 학습 과정에서 최소화 대상이 되는 손실 함수값이나 평가 지표로써 출력 유형에 따라 관찰의 대상이 되기도 한다. 하지만 이 값은 실행 결과로써 얻어 지므로 좋은 값이 나오도록 할뿐 달리 손 댈 방법이 없다.

3. 계산 결과 값

순천파나 역전파에 계속해서 사용되는 값. 퍼셉트론의 가중치나 편향이 여기에 속하게 된다. 딥러닝의 알고리즘에서 이것은 핵심 요소이다.

4. 하이퍼 파라미터

학습률이나 학습횟수, 미니배치 크기같은 모델의 구조나 학습 과정에 영향을 미치는 각종 상수값이다. 실행동안은 값이 변하지 않지만 실행 전후로 값을 변경하여 개발자가 원하는 결과를 얻기까지 수정할 수 있다. 이것은 개발마자 미리 정해줘야하는 값으로 많은 이해화 경험이 필요하다

# 1.8 비선형 정보와 원-핫 벡터 표현

아발로니 데이터셋에서는 암, 수, 유충을 숫자가 아닌 F, M, I로 표시한다. 하지만 신경망에서는 이 정보를 처리하기 위해서는 숫자로 표현해야 한다. 따라서 각각의 정보를 0, 1, 2로 표현한다.

영어로 표시하나 숫자로 표시하나 여기에서는 타당한 선형적인 의미를 찾기 어렵다. 따라서 암, 수, 유충을 세개의 입력 항으로 분할하여 각각 0이나 1로 표현한다. 비선형 정보를 이렇게 항목별로 분할하여 해당 항목만 1, 나머지는 0 으로 나타내는 방식을 **'원-핫 벡터'**표현이라고 한다.'

단층 퍼셉트론에서는 비선형적 데이터를 사용하지 않기 때문에 입력 데이터에 비선형적 특징을 발견할 수 없다. 하지만 다층 퍼셉트론 이상에서는 입력이 비선형적 특성을 띠는지 확인하는 수고를 덜어줄 수 있으며, 단층 퍼셉트의 약점을 완하시켜줄 수 있기 때문에 원-핫 벡터 표현으로 변환을 한다.

# 1.9 코드

```python
import numpy as np
import csv
import time

RND_MEAN = 0
RND_STD = 0.0030
LEARNING_RATE = 0.1
np.random.seed(1234)												# 랜덤 시드값 지정
def randomize(): np.random.seed(time.time())


def main():
    global weight, bias
    abalone_exec()
    print(weight)
    print(bias)


def abalone_exec(epoch_count=100, mb_size=100, report=20):			# 기본 매개변수 지정
    load_abalone_dataset()
    init_model()
    train_and_test(epoch_count, mb_size, report)


def load_abalone_dataset():
    with open('../../../Dataset/abalone/abalone.csv') as csvfile:	# csv파일 불러오기
        csvreader = csv.reader(csvfile)								# csv파일 읽기
        next(csvreader, None)										# csv파일의 맨 첫줄은 카테고리이기 때문에 다음줄부터 읽어오는 것
        rows = []
        for row in csvreader:
            rows.append(row)										# csv파일의 row를 읽어서 저장
        global data, input_cnt, output_cnt
        input_cnt, output_cnt = 10, 1								# 입력 데이터와 출력 데이터의 크기 지정, 입력 데이터 왜 10???
        data = np.zeros([len(rows), input_cnt+output_cnt])			# data 크기를 저렇게 지정하는 이유 ????

        for n, row in enumerate(rows):								# 원-핫 벡터 표현하는 것
            if row[0] == 'I': data[n, 0] = 1
            if row[0] == 'M': data[n, 1] = 1
            if row[0] == 'F': data[n, 2] = 1
            data[n, 3:] = row[1:]


def init_model():
    global weight, bias, input_cnt, output_cnt
    																		# np.random.normal(loc, scale, size)
    weight = np.random.normal(RND_MEAN, RND_STD, [input_cnt, output_cnt])	# 정규분포를 갖는 난수 값으로 초기화 
    																		# RND_MEAN < weight < RND_STD
    bias = np.zeros([output_cnt])											# 편향 0으로 초기화


def train_and_test(epoch_count, mb_size, report):
    step_count = arrange_data(mb_size)
    test_x, test_y = get_test_data()

    for epoch in range(epoch_count):
        losses, accs = [], []

        for n in range(step_count):
            train_x, train_y = get_train_data(mb_size, n)
            loss, acc = run_train(train_x, train_y)
            losses.append(loss)
            accs.append(acc)

        if report > 0 and (epoch+1) % report == 0:							# 보고 주기 확인
            acc = run_test(test_x, test_y)
            print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'.
                  format(epoch+1, np.mean(losses), np.mean(accs), acc))

    final_acc = run_test(test_x, test_y)
    print('\nFinal Test: final accuracy = {:5.3f}'.format(final_acc))


def arrange_data(mb_size):
    global data, shuffle_map, test_begin_idx
    shuffle_map = np.arange(data.shape[0])									# 1부터 data.shape[0]까지 array 설정
    np.random.shuffle(shuffle_map)											# 섞기
    step_count = int(data.shape[0] * 0.8) // mb_size						
    test_begin_idx = step_count * mb_size
    return step_count


def get_test_data():
    global data, shuffle_map, test_begin_idx, output_cnt
    test_data = data[shuffle_map[test_begin_idx:]]
    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]			# 입력백터, 정답벡터 반환


def get_train_data(mb_size, nth):
    global data, shuffle_map, test_begin_idx, output_cnt
    if nth == 0:															# 각 에폭의 첫번째 순서
        np.random.shuffle(shuffle_map[:test_begin_idx])						# 배열 섞기
    train_data = data[shuffle_map[mb_size*nth:mb_size*(nth+1)]]				
    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]


def run_train(x, y):
    output, aux_nn = forward_neuralnet(x)
    loss, aux_pp = forward_postproc(output, y)								# 순전파 처리
    accuracy = eval_accuracy(output, y)

    G_loss = 1.0
    G_output = backprop_postproc(G_loss, aux_pp)
    backprop_neuralnet(G_output, aux_nn)									# 역전파 처리

    return loss, accuracy


def run_test(x, y):
    output, _ = forward_neuralnet(x)										
    accuracy = eval_accuracy(output, y)
    return accuracy


def forward_neuralnet(x):
    global weight, bias
    output = np.matmul(x, weight) + bias									# 가중치곱하고 편향벡터 더하기
    return output, x


def backprop_neuralnet(G_output, x):
    global weight, bias
    g_output_w = x.transpose()

    G_w = np.matmul(g_output_w, G_output)
    G_b = np.sum(G_output, axis=0)

    weight -= LEARNING_RATE * G_w
    bias -= LEARNING_RATE * G_b


def forward_postproc(output, y):
    diff = output - y
    square = np.square(diff)
    loss = np.mean(square)													# 평균제곱오차
    return loss, diff


def backprop_postproc(G_loss, diff):
    shape = diff.shape

    g_loss_square = np.ones(shape) / np.prod(shape)
    g_square_diff = 2 * diff
    g_diff_output = 1

    G_square = g_loss_square * G_loss
    G_diff = g_square_diff * G_square
    G_output = g_diff_output * G_diff

    return G_output


def eval_accuracy(output, y):
    mdiff = np.mean(np.abs((output - y)/y))
    return 1 - mdiff


if __name__ == "__main__":
    main()
```

